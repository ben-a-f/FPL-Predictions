{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9127605a-d777-41bc-b51c-96ddd03b5b50",
   "metadata": {},
   "source": [
    "# FPL Model Building\n",
    "We'll build a model using Keras and set it up as a package so we can train both locally and on the cloud.\n",
    "\n",
    "We need to create a model as a Python package, so we'll need an `__init__.py` to identify the directory as a package, a `model.py` to hold the model code, and a `task.py` to pass command line parameters to our model.We need to create a model as a Python package, so we'll need an `__init__.py` to identify the directory as a package, a `model.py` to hold the model code, and a `task.py` to pass command line parameters to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c0b829-b8c0-4530-b905-334ee6bc00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.10.1\n",
      "tensorflow-cloud==0.1.16\n",
      "tensorflow-datasets==4.8.2\n",
      "tensorflow-estimator==2.10.0\n",
      "tensorflow-hub==0.13.0\n",
      "tensorflow-io==0.27.0\n",
      "tensorflow-io-gcs-filesystem==0.27.0\n",
      "tensorflow-metadata==1.11.0\n",
      "tensorflow-probability==0.19.0\n",
      "tensorflow-serving-api==2.10.1\n",
      "tensorflow-transform==1.11.0\n",
      "google-cloud-storage==2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow || pip install tensorflow\n",
    "!pip freeze | grep google-cloud-storage || pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d5e547-17d5-4e68-86dc-5226a1445800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba277ae-1bb6-490a-95e9-dae980bc4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=bf-fpl-pred-080723\n",
      "env: BUCKET=bf-fpl-pred-080723\n",
      "env: REGION=europe-west1\n",
      "env: OUTDIR=gs://bf-fpl-pred-080723/fpl/data\n",
      "env: TFVERSION=2.8\n"
     ]
    }
   ],
   "source": [
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"europe-west1\"\n",
    "\n",
    "OUTDIR = f\"gs://{BUCKET}/fpl/data\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env OUTDIR=$OUTDIR\n",
    "%env TFVERSION=2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db36998-a471-48d6-a944-41493020a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/trainers/mid/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/__init__.py\n",
    "# Empty init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6991035-20d6-4b40-9a5c-29bbfe735cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/trainers/mid/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/model.py\n",
    "\"\"\"Data prep, train and evaluate DNN model.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks, models\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "# TODO: Parametrise the column list to use the same package for all positions.\n",
    "CSV_COLUMNS = [\n",
    "    \"hash_id\",\n",
    "    \"element_code\",\n",
    "    \"season_name\",\n",
    "    \"next_season_points\",\n",
    "    \"minutes\",\n",
    "    \"goals_scored\",\n",
    "    \"assists\",\n",
    "    \"clean_sheets\",\n",
    "    \"penalties_missed\",\n",
    "    \"bps\",\n",
    "    \"yellow_threshold\",\n",
    "    \"red_cards\",\n",
    "    \"own_goals\",\n",
    "    \"influence\",\n",
    "    \"creativity\",\n",
    "    \"threat\",\n",
    "    \"start_cost\",\n",
    "    \"end_cost\"\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"next_season_points\"\n",
    "DEFAULTS = [\"\", [0.0], \"\", [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "UNWANTED_COLS = [\"hash_id\", \"element_code\", \"season_name\"]\n",
    "\n",
    "INPUT_COLS = [\n",
    "    c for c in CSV_COLUMNS if c != LABEL_COLUMN and c not in UNWANTED_COLS\n",
    "]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "        shuffle_buffer_size=1000000,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nnsize, lr):\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # Concatenate numeric inputs\n",
    "    dnn_inputs = Concatenate()(list(inputs.values()))\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = Dense(nodes, activation=\"relu\", name=f\"h{layer}\")(x)\n",
    "    output = Dense(1, name=\"next_sason_points\")(x)\n",
    "\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    lr_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    nnsize = [int(s) for s in hparams[\"nnsize\"].split()]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "\n",
    "    model_export_path = os.path.join(output_dir, \"savedmodel\")\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    model = build_dnn_model(nnsize, lr)\n",
    "    logging.info(model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, save_weights_only=True, verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb],\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    model.save(model_export_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b0ce22-2b7d-41dc-aaf4-562bad203b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/trainers/mid/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/task.py\n",
    "\"\"\"Argument definitions for model training code in `trainer.model`.\"\"\"\n",
    "# TODO: Add CSV_COLUMNS.\n",
    "\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        default=\"32 8\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", help=\"learning rate for optimizer\", type=float, default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    model.train_and_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01bbb5d7-3563-4188-ab4b-c02edf849b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python3: Error while finding module specification for 'trainer.task' (ModuleNotFoundError: No module named 'trainer')\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nEVAL_DATA_PATH=gs://${BUCKET}/fpl/data/mid-test*\\nTRAIN_DATA_PATH=gs://${BUCKET}/fpl/data/mid-train*\\nOUTPUT_DIR=./models/runs/mid\\n\\ntest ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\\n    \\n# Run the trainer module package locally with 1 eval\\n\\npython3 -m trainer.task \\\\\\n--eval_data_path $EVAL_DATA_PATH \\\\\\n--output_dir $OUTPUT_DIR \\\\\\n--train_data_path $TRAIN_DATA_PATH \\\\\\n--batch_size 5 \\\\\\n--num_examples_to_train_on 100 \\\\\\n--num_evals 1 \\\\\\n--lr 0.001 \\\\\\n--nnsize \"32 8\"\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/217076403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nEVAL_DATA_PATH=gs://${BUCKET}/fpl/data/mid-test*\\nTRAIN_DATA_PATH=gs://${BUCKET}/fpl/data/mid-train*\\nOUTPUT_DIR=./models/runs/mid\\n\\ntest ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\\n    \\n# Run the trainer module package locally with 1 eval\\n\\npython3 -m trainer.task \\\\\\n--eval_data_path $EVAL_DATA_PATH \\\\\\n--output_dir $OUTPUT_DIR \\\\\\n--train_data_path $TRAIN_DATA_PATH \\\\\\n--batch_size 5 \\\\\\n--num_examples_to_train_on 100 \\\\\\n--num_evals 1 \\\\\\n--lr 0.001 \\\\\\n--nnsize \"32 8\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nEVAL_DATA_PATH=gs://${BUCKET}/fpl/data/mid-test*\\nTRAIN_DATA_PATH=gs://${BUCKET}/fpl/data/mid-train*\\nOUTPUT_DIR=./models/runs/mid\\n\\ntest ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\\n    \\n# Run the trainer module package locally with 1 eval\\n\\npython3 -m trainer.task \\\\\\n--eval_data_path $EVAL_DATA_PATH \\\\\\n--output_dir $OUTPUT_DIR \\\\\\n--train_data_path $TRAIN_DATA_PATH \\\\\\n--batch_size 5 \\\\\\n--num_examples_to_train_on 100 \\\\\\n--num_evals 1 \\\\\\n--lr 0.001 \\\\\\n--nnsize \"32 8\"\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "EVAL_DATA_PATH=gs://${BUCKET}/fpl/data/mid-test*\n",
    "TRAIN_DATA_PATH=gs://${BUCKET}/fpl/data/mid-train*\n",
    "OUTPUT_DIR=./models/runs/mid\n",
    "\n",
    "test ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\n",
    "    \n",
    "# Run the trainer module package locally with 1 eval\n",
    "\n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize \"32 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c347a5-5b0e-4d38-8ec3-fd2f862cbe37",
   "metadata": {},
   "source": [
    "Now we know it works, we can package up the code as a source distribution to be able to run it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b589ffe-294a-4903-a7ad-b2a3a09e7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./models/setups/mid/setup.py\n",
    "\"\"\"Using `setuptools` to create a source distribution.\"\"\"\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name=\"mid_trainer\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Midfielder model training application.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c44e6-4015-4eda-8ea0-16fb1574efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python ./models/setups/mid/setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8399a-717b-495c-b59b-98b02cd00f43",
   "metadata": {},
   "source": [
    "Move files over to a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5aedad-327f-4e6b-8c2e-a587bc460043",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp dist/mid_trainer-0.1.tar.gz gs://$BUCKET/fpl/trainers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8393612-cccc-470c-8399-63557feb243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# # Output directory and jobID\n",
    "# TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "# OUTDIR=gs://${BUCKET}/fpl/trained-models/mid_model_$TIMESTAMP\n",
    "# JOB_NAME=mid_$TIMESTAMP\n",
    "# echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "# PYTHON_PACKAGE_URIS=gs://${BUCKET}/fpl/trainers/mid_trainer-0.1.tar.gz\n",
    "# MACHINE_TYPE=n1-standard-4\n",
    "# REPLICA_COUNT=1\n",
    "# PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "# PYTHON_MODULE=trainer.task\n",
    "\n",
    "# # Model and training hyperparameters\n",
    "# BATCH_SIZE=50\n",
    "# NUM_EXAMPLES_TO_TRAIN_ON=5000\n",
    "# NUM_EVALS=100\n",
    "# LR=0.001\n",
    "# NNSIZE=\"32 8\"\n",
    "\n",
    "\n",
    "# # GCS paths\n",
    "# GCS_PROJECT_PATH=gs://$BUCKET\n",
    "# DATA_PATH=$GCS_PROJECT_PATH/fpl/data\n",
    "# TRAIN_DATA_PATH=$DATA_PATH/mid-train*\n",
    "# EVAL_DATA_PATH=$DATA_PATH/mid-tst*\n",
    "\n",
    "# WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "# replica-count=$REPLICA_COUNT,\\\n",
    "# executor-image-uri=$PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\\\n",
    "# python-module=$PYTHON_MODULE\"\n",
    "\n",
    "# ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "# --output_dir=$OUTDIR,\\\n",
    "# --train_data_path=$TRAIN_DATA_PATH,\\\n",
    "# --batch_size=$BATCH_SIZE,\\\n",
    "# --num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "# --num_evals=$NUM_EVALS,\\\n",
    "# --lr=$LR,\\\n",
    "# --nnsize=$NNSIZE\"\n",
    "\n",
    "# # Create a custom job\n",
    "\n",
    "# gcloud ai custom-jobs create \\\n",
    "#   --region=${REGION} \\\n",
    "#   --display-name=$JOB_NAME \\\n",
    "#   --python-package-uris=$PYTHON_PACKAGE_URIS \\\n",
    "#   --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "#   --args=\"$ARGS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754b38c-f0a7-43a9-9cf4-fa3b709ade70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
