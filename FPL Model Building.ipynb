{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9127605a-d777-41bc-b51c-96ddd03b5b50",
   "metadata": {},
   "source": [
    "# FPL Model Building\n",
    "We'll build a model using Keras and set it up as a package so we can train both locally and on the cloud.\n",
    "\n",
    "We need to create a model as a Python package, so we'll need an `__init__.py` to identify the directory as a package, a `model.py` to hold the model code, and a `task.py` to pass command line parameters to our model.We need to create a model as a Python package, so we'll need an `__init__.py` to identify the directory as a package, a `model.py` to hold the model code, and a `task.py` to pass command line parameters to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c0b829-b8c0-4530-b905-334ee6bc00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.10.1\n",
      "tensorflow-cloud==0.1.16\n",
      "tensorflow-datasets==4.8.2\n",
      "tensorflow-estimator==2.10.0\n",
      "tensorflow-hub==0.13.0\n",
      "tensorflow-io==0.27.0\n",
      "tensorflow-io-gcs-filesystem==0.27.0\n",
      "tensorflow-metadata==1.11.0\n",
      "tensorflow-probability==0.19.0\n",
      "tensorflow-serving-api==2.10.1\n",
      "tensorflow-transform==1.11.0\n",
      "google-cloud-storage==2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow || pip install tensorflow\n",
    "!pip freeze | grep google-cloud-storage || pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d5e547-17d5-4e68-86dc-5226a1445800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba277ae-1bb6-490a-95e9-dae980bc4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=bf-fpl-pred-080723\n",
      "env: BUCKET=bf-fpl-pred-080723\n",
      "env: REGION=europe-west1\n",
      "env: OUTDIR=gs://bf-fpl-pred-080723/fpl/data\n",
      "env: TFVERSION=2.8\n"
     ]
    }
   ],
   "source": [
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"europe-west1\"\n",
    "\n",
    "OUTDIR = f\"gs://{BUCKET}/fpl/data\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env OUTDIR=$OUTDIR\n",
    "%env TFVERSION=2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db36998-a471-48d6-a944-41493020a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/trainers/mid/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/__init__.py\n",
    "# Empty init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6991035-20d6-4b40-9a5c-29bbfe735cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/trainers/mid/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/model.py\n",
    "\"\"\"Data prep, train and evaluate DNN model.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks, models\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "# TODO: Parametrise the column list to use the same package for all positions.\n",
    "CSV_COLUMNS = [\n",
    "    \"hash_id\",\n",
    "    \"element_code\",\n",
    "    \"season_name\",\n",
    "    \"next_season_points\",\n",
    "    \"minutes\",\n",
    "    \"goals_scored\",\n",
    "    \"assists\",\n",
    "    \"clean_sheets\",\n",
    "    \"penalties_missed\",\n",
    "    \"bps\",\n",
    "    \"yellow_threshold\",\n",
    "    \"red_cards\",\n",
    "    \"own_goals\",\n",
    "    \"influence\",\n",
    "    \"creativity\",\n",
    "    \"threat\",\n",
    "    \"start_cost\",\n",
    "    \"end_cost\"\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"next_season_points\"\n",
    "DEFAULTS = [\"\", [0.0], \"\", [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "UNWANTED_COLS = [\"hash_id\", \"element_code\", \"season_name\"]\n",
    "\n",
    "INPUT_COLS = [\n",
    "    c for c in CSV_COLUMNS if c != LABEL_COLUMN and c not in UNWANTED_COLS\n",
    "]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "        shuffle_buffer_size=1000000,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nnsize, lr):\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # Concatenate numeric inputs\n",
    "    dnn_inputs = Concatenate()(list(inputs.values()))\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = Dense(nodes, activation=\"relu\", name=f\"h{layer}\")(x)\n",
    "    output = Dense(1, name=\"next_sason_points\")(x)\n",
    "\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    lr_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    nnsize = [int(s) for s in hparams[\"nnsize\"].split()]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "\n",
    "    model_export_path = os.path.join(output_dir, \"savedmodel\")\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    model = build_dnn_model(nnsize, lr)\n",
    "    logging.info(model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, save_weights_only=True, verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb],\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    model.save(model_export_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41b0ce22-2b7d-41dc-aaf4-562bad203b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./models/trainers/mid/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/task.py\n",
    "\"\"\"Argument definitions for model training code in `trainer.model`.\"\"\"\n",
    "# TODO: Add CSV_COLUMNS.\n",
    "\n",
    "import argparse\n",
    "\n",
    "from . import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        default=\"32 8\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", help=\"learning rate for optimizer\", type=float, default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    model.train_and_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bbb5d7-3563-4188-ab4b-c02edf849b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " minutes (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " goals_scored (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " assists (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " clean_sheets (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " penalties_missed (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " bps (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " yellow_threshold (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " red_cards (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " own_goals (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " influence (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " creativity (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " threat (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " start_cost (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " end_cost (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14)           0           ['minutes[0][0]',                \n",
      "                                                                  'goals_scored[0][0]',           \n",
      "                                                                  'assists[0][0]',                \n",
      "                                                                  'clean_sheets[0][0]',           \n",
      "                                                                  'penalties_missed[0][0]',       \n",
      "                                                                  'bps[0][0]',                    \n",
      "                                                                  'yellow_threshold[0][0]',       \n",
      "                                                                  'red_cards[0][0]',              \n",
      "                                                                  'own_goals[0][0]',              \n",
      "                                                                  'influence[0][0]',              \n",
      "                                                                  'creativity[0][0]',             \n",
      "                                                                  'threat[0][0]',                 \n",
      "                                                                  'start_cost[0][0]',             \n",
      "                                                                  'end_cost[0][0]']               \n",
      "                                                                                                  \n",
      " h0 (Dense)                     (None, 32)           480         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " h1 (Dense)                     (None, 8)            264         ['h0[0][0]']                     \n",
      "                                                                                                  \n",
      " next_sason_points (Dense)      (None, 1)            9           ['h1[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 753\n",
      "Trainable params: 753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Epoch 1: saving model to ./models/runs/mid/checkpoints\n",
      "20/20 - 2s - loss: 90155.4219 - rmse: 287.7122 - mse: 90155.4219 - val_loss: 34830.5859 - val_rmse: 183.2169 - val_mse: 34830.5859 - 2s/epoch - 121ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 11:53:44.751735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 11:53:44.952423: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-04 11:53:48.153129: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-04 11:53:48.153290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-04 11:53:48.153321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-08-04 11:53:51.280139: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-04 11:53:51.280185: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-04 11:53:51.280222: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a3fbd3edaae2): /proc/driver/nvidia/version does not exist\n",
      "2023-08-04 11:53:51.280487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "EVAL_DATA_PATH=gs://${BUCKET}/fpl/data/mid-test*\n",
    "TRAIN_DATA_PATH=gs://${BUCKET}/fpl/data/mid-train*\n",
    "OUTPUT_DIR=./models/runs/mid\n",
    "\n",
    "test ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/models/trainers/mid\n",
    "    \n",
    "# Run the trainer module package locally with 1 eval\n",
    "\n",
    "python3 -m models.trainers.mid.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize \"32 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c347a5-5b0e-4d38-8ec3-fd2f862cbe37",
   "metadata": {},
   "source": [
    "Now we know it works, we can package up the code as a source distribution to be able to run it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b589ffe-294a-4903-a7ad-b2a3a09e7727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./models/trainers/mid/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/setup.py\n",
    "\"\"\"Using `setuptools` to create a source distribution.\"\"\"\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name=\"mid-trainer\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Midfielder model training application.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da1c44e6-4015-4eda-8ea0-16fb1574efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing mid_trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to mid_trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to mid_trainer.egg-info/top_level.txt\n",
      "reading manifest file 'mid_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'mid_trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating mid-trainer-0.1\n",
      "creating mid-trainer-0.1/mid_trainer.egg-info\n",
      "creating mid-trainer-0.1/mid_trainer.egg-info/.ipynb_checkpoints\n",
      "copying files to mid-trainer-0.1...\n",
      "copying setup.py -> mid-trainer-0.1\n",
      "copying mid_trainer.egg-info/PKG-INFO -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/SOURCES.txt -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/dependency_links.txt -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/top_level.txt -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/.ipynb_checkpoints/PKG-INFO-checkpoint -> mid-trainer-0.1/mid_trainer.egg-info/.ipynb_checkpoints\n",
      "copying mid_trainer.egg-info/.ipynb_checkpoints/SOURCES-checkpoint.txt -> mid-trainer-0.1/mid_trainer.egg-info/.ipynb_checkpoints\n",
      "copying mid_trainer.egg-info/.ipynb_checkpoints/dependency_links-checkpoint.txt -> mid-trainer-0.1/mid_trainer.egg-info/.ipynb_checkpoints\n",
      "copying mid_trainer.egg-info/.ipynb_checkpoints/top_level-checkpoint.txt -> mid-trainer-0.1/mid_trainer.egg-info/.ipynb_checkpoints\n",
      "Writing mid-trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'mid-trainer-0.1' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd models/trainers/mid\n",
    "python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8399a-717b-495c-b59b-98b02cd00f43",
   "metadata": {},
   "source": [
    "Move files over to a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de5aedad-327f-4e6b-8c2e-a587bc460043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/mid-trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  912.0 B/  912.0 B]                                                \n",
      "Operation completed over 1 objects/912.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp models/trainers/mid/dist/mid-trainer-0.1.tar.gz gs://$BUCKET/fpl/trainers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8393612-cccc-470c-8399-63557feb243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bf-fpl-pred-080723/fpl/trained-models/mid_model_20230804_122815 europe-west1 mid_20230804_122815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://europe-west1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/343566520815/locations/europe-west1/customJobs/792417205504442368] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/343566520815/locations/europe-west1/customJobs/792417205504442368\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/343566520815/locations/europe-west1/customJobs/792417205504442368\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Output directory and jobID\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/fpl/trained-models/mid_model_$TIMESTAMP\n",
    "JOB_NAME=mid_$TIMESTAMP\n",
    "echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "PYTHON_PACKAGE_URIS=gs://${BUCKET}/fpl/trainers/mid-trainer-0.1.tar.gz\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=5000\n",
    "NUM_EVALS=100\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET\n",
    "DATA_PATH=$GCS_PROJECT_PATH/fpl/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/mid-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/mid-test*\n",
    "\n",
    "WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "replica-count=$REPLICA_COUNT,\\\n",
    "executor-image-uri=$PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\\\n",
    "python-module=$PYTHON_MODULE\"\n",
    "\n",
    "ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "--output_dir=$OUTDIR,\\\n",
    "--train_data_path=$TRAIN_DATA_PATH,\\\n",
    "--batch_size=$BATCH_SIZE,\\\n",
    "--num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "--num_evals=$NUM_EVALS,\\\n",
    "--lr=$LR,\\\n",
    "--nnsize=$NNSIZE\"\n",
    "\n",
    "# Create a custom job\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --python-package-uris=$PYTHON_PACKAGE_URIS \\\n",
    "  --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "  --args=\"$ARGS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170f9a1-acac-439b-9667-6e66b5ce8ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
