{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9127605a-d777-41bc-b51c-96ddd03b5b50",
   "metadata": {},
   "source": [
    "# FPL Model Building\n",
    "We'll build a model using Keras and set it up as a package so we can train both locally and on the cloud.\n",
    "\n",
    "We need to create a model as a Python package, so we'll need an `__init__.py` to identify the directory as a package, a `model.py` to hold the model code, and a `task.py` to pass command line parameters to our model.We need to create a model as a Python package, so we'll need an `__init__.py` to identify the directory as a package, a `model.py` to hold the model code, and a `task.py` to pass command line parameters to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c0b829-b8c0-4530-b905-334ee6bc00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.10.1\n",
      "tensorflow-cloud==0.1.16\n",
      "tensorflow-datasets==4.8.2\n",
      "tensorflow-estimator==2.10.0\n",
      "tensorflow-hub==0.13.0\n",
      "tensorflow-io==0.27.0\n",
      "tensorflow-io-gcs-filesystem==0.27.0\n",
      "tensorflow-metadata==1.11.0\n",
      "tensorflow-probability==0.19.0\n",
      "tensorflow-serving-api==2.10.1\n",
      "tensorflow-transform==1.11.0\n",
      "google-cloud-storage==2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow || pip install tensorflow\n",
    "!pip freeze | grep google-cloud-storage || pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d5e547-17d5-4e68-86dc-5226a1445800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba277ae-1bb6-490a-95e9-dae980bc4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=bf-fpl-pred-080723\n",
      "env: BUCKET=bf-fpl-pred-080723\n",
      "env: REGION=europe-west1\n",
      "env: OUTDIR=gs://bf-fpl-pred-080723/fpl/data\n",
      "env: TFVERSION=2.8\n"
     ]
    }
   ],
   "source": [
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"europe-west1\"\n",
    "\n",
    "OUTDIR = f\"gs://{BUCKET}/fpl/data\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env OUTDIR=$OUTDIR\n",
    "%env TFVERSION=2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db36998-a471-48d6-a944-41493020a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./models/trainers/mid/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/__init__.py\n",
    "# Empty init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6991035-20d6-4b40-9a5c-29bbfe735cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./models/trainers/mid/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/model.py\n",
    "\"\"\"Data prep, train and evaluate DNN model.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks, models\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "# TODO: Parametrise the column list to use the same package for all positions.\n",
    "CSV_COLUMNS = [\n",
    "    \"hash_id\",\n",
    "    \"element_code\",\n",
    "    \"season_name\",\n",
    "    \"next_season_points\",\n",
    "    \"minutes\",\n",
    "    \"goals_scored\",\n",
    "    \"assists\",\n",
    "    \"clean_sheets\",\n",
    "    \"penalties_missed\",\n",
    "    \"bps\",\n",
    "    \"yellow_threshold\",\n",
    "    \"red_cards\",\n",
    "    \"own_goals\",\n",
    "    \"influence\",\n",
    "    \"creativity\",\n",
    "    \"threat\",\n",
    "    \"start_cost\",\n",
    "    \"end_cost\"\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"next_season_points\"\n",
    "DEFAULTS = [\"\", [0.0], \"\", [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "UNWANTED_COLS = [\"hash_id\", \"element_code\", \"season_name\"]\n",
    "\n",
    "INPUT_COLS = [\n",
    "    c for c in CSV_COLUMNS if c != LABEL_COLUMN and c not in UNWANTED_COLS\n",
    "]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "        shuffle_buffer_size=1000000,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nnsize, lr):\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # Concatenate numeric inputs\n",
    "    dnn_inputs = Concatenate()(list(inputs.values()))\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = Dense(nodes, activation=\"relu\", name=f\"h{layer}\")(x)\n",
    "    output = Dense(1, name=\"next_sason_points\")(x)\n",
    "\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    lr_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    nnsize = [int(s) for s in hparams[\"nnsize\"].split()]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "\n",
    "    model_export_path = os.path.join(output_dir, \"savedmodel\")\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    model = build_dnn_model(nnsize, lr)\n",
    "    logging.info(model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, save_weights_only=True, verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb],\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    model.save(model_export_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b0ce22-2b7d-41dc-aaf4-562bad203b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./models/trainers/mid/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/mid/task.py\n",
    "\"\"\"Argument definitions for model training code in `trainer.model`.\"\"\"\n",
    "# TODO: Add CSV_COLUMNS.\n",
    "\n",
    "import argparse\n",
    "\n",
    "from mid import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        default=\"32 8\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", help=\"learning rate for optimizer\", type=float, default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    model.train_and_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c347a5-5b0e-4d38-8ec3-fd2f862cbe37",
   "metadata": {},
   "source": [
    "Next we package up the code as a source distribution to be able to run it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b589ffe-294a-4903-a7ad-b2a3a09e7727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./models/trainers/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/trainers/setup.py\n",
    "\"\"\"Using `setuptools` to create a source distribution.\"\"\"\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name=\"mid-trainer\",\n",
    "    version=\"0.1\",\n",
    "    packages=['mid'],\n",
    "    include_package_data=True,\n",
    "    description=\"Midfielder model training application.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da1c44e6-4015-4eda-8ea0-16fb1574efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing mid_trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to mid_trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to mid_trainer.egg-info/top_level.txt\n",
      "reading manifest file 'mid_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'mid_trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating mid-trainer-0.1\n",
      "creating mid-trainer-0.1/mid\n",
      "creating mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying files to mid-trainer-0.1...\n",
      "copying setup.py -> mid-trainer-0.1\n",
      "copying mid/__init__.py -> mid-trainer-0.1/mid\n",
      "copying mid/model.py -> mid-trainer-0.1/mid\n",
      "copying mid/task.py -> mid-trainer-0.1/mid\n",
      "copying mid_trainer.egg-info/PKG-INFO -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/SOURCES.txt -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/dependency_links.txt -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "copying mid_trainer.egg-info/top_level.txt -> mid-trainer-0.1/mid_trainer.egg-info\n",
      "Writing mid-trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'mid-trainer-0.1' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd models/trainers\n",
    "python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8399a-717b-495c-b59b-98b02cd00f43",
   "metadata": {},
   "source": [
    "Move files over to a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5aedad-327f-4e6b-8c2e-a587bc460043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://models/trainers/dist/mid-trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.6 KiB/  2.6 KiB]                                                \n",
      "Operation completed over 1 objects/2.6 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp models/trainers/dist/mid-trainer-0.1.tar.gz gs://$BUCKET/fpl/trainers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8393612-cccc-470c-8399-63557feb243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bf-fpl-pred-080723/fpl/trained-models/mid_model_20230805_164422 europe-west1 mid_20230805_164422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://europe-west1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/343566520815/locations/europe-west1/customJobs/8750713153672708096] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/343566520815/locations/europe-west1/customJobs/8750713153672708096\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/343566520815/locations/europe-west1/customJobs/8750713153672708096\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Output directory and jobID\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/fpl/trained-models/mid_model_$TIMESTAMP\n",
    "JOB_NAME=mid_$TIMESTAMP\n",
    "echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "PYTHON_PACKAGE_URIS=gs://${BUCKET}/fpl/trainers/mid-trainer-0.1.tar.gz\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=mid.task\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=5000\n",
    "NUM_EVALS=100\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET\n",
    "DATA_PATH=$GCS_PROJECT_PATH/fpl/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/mid-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/mid-test*\n",
    "\n",
    "WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "replica-count=$REPLICA_COUNT,\\\n",
    "executor-image-uri=$PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\\\n",
    "python-module=$PYTHON_MODULE\"\n",
    "\n",
    "ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "--output_dir=$OUTDIR,\\\n",
    "--train_data_path=$TRAIN_DATA_PATH,\\\n",
    "--batch_size=$BATCH_SIZE,\\\n",
    "--num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "--num_evals=$NUM_EVALS,\\\n",
    "--lr=$LR,\\\n",
    "--nnsize=$NNSIZE\"\n",
    "\n",
    "# Create a custom job\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --python-package-uris=$PYTHON_PACKAGE_URIS \\\n",
    "  --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "  --args=\"$ARGS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170f9a1-acac-439b-9667-6e66b5ce8ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
